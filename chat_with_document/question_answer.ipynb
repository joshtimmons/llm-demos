{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshtimmons/llm-demos/blob/main/chat_with_document/question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9528f94",
      "metadata": {},
      "source": [
        "# How to \"Chat\" with a document\n",
        "\n",
        "This is the accompanying material for the 11/8 AI session. \n",
        "\n",
        "In this session we will learn about:\n",
        "1. text embeddings\n",
        "2. vector stores\n",
        "3. document chunking\n",
        "4. document retrieval based on a prompted question\n",
        "5. converting a retrieved document into an \"answer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ji2q5P50hyC6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji2q5P50hyC6",
        "outputId": "5d2bcce4-0150-4db4-c557-e8761b1cd974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.331-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.16-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting anyio<4.0 (from langchain)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.60-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (1.26.1)\n",
            "Collecting pydantic<3,>=1 (from langchain)\n",
            "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers) (2.1.0)\n",
            "Collecting torchvision (from sentence_transformers)\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers) (1.3.1)\n",
            "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers) (1.11.3)\n",
            "Collecting nltk (from sentence_transformers)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (4.8.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb)\n",
            "  Downloading grpcio-1.59.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1->langchain)\n",
            "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers) (12.2.140)\n",
            "Collecting click<9.0.0,>=7.1.1 (from typer>=0.9.0->chromadb)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.0.331-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.4.16-py3-none-any.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.1/496.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.59.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.0.60-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m853.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.2/613.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Building wheels for collected packages: sentence_transformers, pypika\n",
            "  Building wheel for sentence_transformers (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=b1f4030c0b109152774f2e7e47f00504c73cf57feb08f4ad42087922d6d5bd5f\n",
            "  Stored in directory: /home/codespace/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=85e4bf352bd35264766ff7b9eb19ee2afa6e5a28cf2dbf685d7868e712d894a1\n",
            "  Stored in directory: /home/codespace/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built sentence_transformers pypika\n",
            "Installing collected packages: sentencepiece, pypika, monotonic, flatbuffers, zipp, wrapt, websockets, uvloop, urllib3, tqdm, safetensors, regex, python-dotenv, pydantic-core, pyasn1, pulsar-client, protobuf, opentelemetry-semantic-conventions, oauthlib, mypy-extensions, multidict, marshmallow, jsonpatch, importlib-resources, humanfriendly, httptools, h11, grpcio, greenlet, frozenlist, click, chroma-hnswlib, cachetools, bcrypt, backoff, async-timeout, anyio, annotated-types, yarl, watchfiles, uvicorn, typing-inspect, typer, starlette, SQLAlchemy, rsa, pydantic, pyasn1-modules, opentelemetry-proto, nltk, importlib-metadata, googleapis-common-protos, deprecated, coloredlogs, aiosignal, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, langsmith, huggingface-hub, google-auth, fastapi, dataclasses-json, aiohttp, torchvision, tokenizers, opentelemetry-sdk, langchain, kubernetes, transformers, opentelemetry-exporter-otlp-proto-grpc, sentence_transformers, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.0.0\n",
            "    Uninstalling anyio-4.0.0:\n",
            "      Successfully uninstalled anyio-4.0.0\n",
            "Successfully installed SQLAlchemy-2.0.23 aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 anyio-3.7.1 async-timeout-4.0.3 backoff-2.2.1 bcrypt-4.0.1 cachetools-5.3.2 chroma-hnswlib-0.7.3 chromadb-0.4.16 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.1 deprecated-1.2.14 fastapi-0.104.1 flatbuffers-23.5.26 frozenlist-1.4.0 google-auth-2.23.4 googleapis-common-protos-1.61.0 greenlet-3.0.1 grpcio-1.59.2 h11-0.14.0 httptools-0.6.1 huggingface-hub-0.17.3 humanfriendly-10.0 importlib-metadata-6.8.0 importlib-resources-6.1.1 jsonpatch-1.33 kubernetes-28.1.0 langchain-0.0.331 langsmith-0.0.60 marshmallow-3.20.1 monotonic-1.6 multidict-6.0.4 mypy-extensions-1.0.0 nltk-3.8.1 oauthlib-3.2.2 onnxruntime-1.16.1 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 posthog-3.0.2 protobuf-4.25.0 pulsar-client-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydantic-2.4.2 pydantic-core-2.10.1 pypika-0.48.9 python-dotenv-1.0.0 regex-2023.10.3 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.14.1 torchvision-0.16.0 tqdm-4.66.1 transformers-4.35.0 typer-0.9.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 wrapt-1.15.0 yarl-1.9.2 zipp-3.17.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain transformers sentence_transformers \"chromadb<=0.4.15\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9908d800",
      "metadata": {},
      "source": [
        "First we're going to do some basic model setup. I'm using the lightweight flan-alpaca-large model that is tuned for assistive generation. \n",
        "\n",
        "This model plays two roles in our QA scenario:\n",
        "1. It will figure out what text in our documents contains content that answers the question\n",
        "2. It will combine the information we collect into a single answer\n",
        "\n",
        "This is a small model by LLM standards at 0.7B parameters, but it's good enough for the job :-) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d5b2d99a",
      "metadata": {
        "id": "d5b2d99a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 2.50k/2.50k [00:00<00:00, 9.98MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 55.0MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 7.36MB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 787/787 [00:00<00:00, 2.54MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [01:10<00:00, 44.7MB/s]\n",
            "Downloading (…)neration_config.json: 100%|██████████| 142/142 [00:00<00:00, 575kB/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/llm-demos/chat_with_document/question_answer.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext2text-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel, tokenizer\u001b[39m=\u001b[39;49mtokenizer, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m local_llm \u001b[39m=\u001b[39m HuggingFacePipeline(pipeline\u001b[39m=\u001b[39mpipe)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1070\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m-> 1070\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39;49mmodel, framework\u001b[39m=\u001b[39;49mframework, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:67\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[1;32m     70\u001b[0m         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     71\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[39melse\u001b[39;00m MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     73\u001b[0m     )\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/base.py:797\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m     \u001b[39mif\u001b[39;00m hf_device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m         \u001b[39m# Take the first device used by `accelerate`.\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "# This code downloads the model from the HuggingFace model hub and loads it into a pipeline.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "model_id = \"declare-lab/flan-alpaca-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30105807",
      "metadata": {},
      "source": [
        "Next we're going to start the process of vectorizing and storing the text.\n",
        "\n",
        "I'm using the first chapter of \"The Wizard of Oz\" by Frank Baum for illustrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "965555f4",
      "metadata": {
        "id": "965555f4"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "PqW5bk6GnEpD",
      "metadata": {
        "id": "PqW5bk6GnEpD"
      },
      "outputs": [],
      "source": [
        "woz = \"\"\"\n",
        "Dorothy lived in the midst of the great Kansas prairies, with Uncle\n",
        "Henry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\n",
        "house was small, for the lumber to build it had to be carried by wagon\n",
        "many miles. There were four walls, a floor and a roof, which made one\n",
        "room; and this room contained a rusty looking cookstove, a cupboard for\n",
        "the dishes, a table, three or four chairs, and the beds. Uncle Henry\n",
        "and Aunt Em had a big bed in one corner, and Dorothy a little bed in\n",
        "another corner. There was no garret at all, and no cellar—except a\n",
        "small hole dug in the ground, called a cyclone cellar, where the family\n",
        "could go in case one of those great whirlwinds arose, mighty enough to\n",
        "crush any building in its path. It was reached by a trap door in the\n",
        "middle of the floor, from which a ladder led down into the small, dark\n",
        "hole.\n",
        "\n",
        "When Dorothy stood in the doorway and looked around, she could see\n",
        "nothing but the great gray prairie on every side. Not a tree nor a\n",
        "house broke the broad sweep of flat country that reached to the edge of\n",
        "the sky in all directions. The sun had baked the plowed land into a\n",
        "gray mass, with little cracks running through it. Even the grass was\n",
        "not green, for the sun had burned the tops of the long blades until\n",
        "they were the same gray color to be seen everywhere. Once the house had\n",
        "been painted, but the sun blistered the paint and the rains washed it\n",
        "away, and now the house was as dull and gray as everything else.\n",
        "\n",
        "When Aunt Em came there to live she was a young, pretty wife. The sun\n",
        "and wind had changed her, too. They had taken the sparkle from her eyes\n",
        "and left them a sober gray; they had taken the red from her cheeks and\n",
        "lips, and they were gray also. She was thin and gaunt, and never smiled\n",
        "now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
        "been so startled by the child’s laughter that she would scream and\n",
        "press her hand upon her heart whenever Dorothy’s merry voice reached\n",
        "her ears; and she still looked at the little girl with wonder that she\n",
        "could find anything to laugh at.\n",
        "\n",
        "Uncle Henry never laughed. He worked hard from morning till night and\n",
        "did not know what joy was. He was gray also, from his long beard to his\n",
        "rough boots, and he looked stern and solemn, and rarely spoke.\n",
        "\n",
        "It was Toto that made Dorothy laugh, and saved her from growing as gray\n",
        "as her other surroundings. Toto was not gray; he was a little black\n",
        "dog, with long silky hair and small black eyes that twinkled merrily on\n",
        "either side of his funny, wee nose. Toto played all day long, and\n",
        "Dorothy played with him, and loved him dearly.\n",
        "\n",
        "Today, however, they were not playing. Uncle Henry sat upon the\n",
        "doorstep and looked anxiously at the sky, which was even grayer than\n",
        "usual. Dorothy stood in the door with Toto in her arms, and looked at\n",
        "the sky too. Aunt Em was washing the dishes.\n",
        "\n",
        "From the far north they heard a low wail of the wind, and Uncle Henry\n",
        "and Dorothy could see where the long grass bowed in waves before the\n",
        "coming storm. There now came a sharp whistling in the air from the\n",
        "south, and as they turned their eyes that way they saw ripples in the\n",
        "grass coming from that direction also.\n",
        "\n",
        "Suddenly Uncle Henry stood up.\n",
        "\n",
        "“There’s a cyclone coming, Em,” he called to his wife. “I’ll go look\n",
        "after the stock.” Then he ran toward the sheds where the cows and\n",
        "horses were kept.\n",
        "\n",
        "Aunt Em dropped her work and came to the door. One glance told her of\n",
        "the danger close at hand.\n",
        "\n",
        "“Quick, Dorothy!” she screamed. “Run for the cellar!”\n",
        "\n",
        "Toto jumped out of Dorothy’s arms and hid under the bed, and the girl\n",
        "started to get him. Aunt Em, badly frightened, threw open the trap door\n",
        "in the floor and climbed down the ladder into the small, dark hole.\n",
        "Dorothy caught Toto at last and started to follow her aunt. When she\n",
        "was halfway across the room there came a great shriek from the wind,\n",
        "and the house shook so hard that she lost her footing and sat down\n",
        "suddenly upon the floor.\n",
        "\n",
        "Then a strange thing happened.\n",
        "\n",
        "The house whirled around two or three times and rose slowly through the\n",
        "air. Dorothy felt as if she were going up in a balloon.\n",
        "\n",
        "The north and south winds met where the house stood, and made it the\n",
        "exact center of the cyclone. In the middle of a cyclone the air is\n",
        "generally still, but the great pressure of the wind on every side of\n",
        "the house raised it up higher and higher, until it was at the very top\n",
        "of the cyclone; and there it remained and was carried miles and miles\n",
        "away as easily as you could carry a feather.\n",
        "\n",
        "It was very dark, and the wind howled horribly around her, but Dorothy\n",
        "found she was riding quite easily. After the first few whirls around,\n",
        "and one other time when the house tipped badly, she felt as if she were\n",
        "being rocked gently, like a baby in a cradle.\n",
        "\n",
        "Toto did not like it. He ran about the room, now here, now there,\n",
        "barking loudly; but Dorothy sat quite still on the floor and waited to\n",
        "see what would happen.\n",
        "\n",
        "Once Toto got too near the open trap door, and fell in; and at first\n",
        "the little girl thought she had lost him. But soon she saw one of his\n",
        "ears sticking up through the hole, for the strong pressure of the air\n",
        "was keeping him up so that he could not fall. She crept to the hole,\n",
        "caught Toto by the ear, and dragged him into the room again, afterward\n",
        "closing the trap door so that no more accidents could happen.\n",
        "\n",
        "Hour after hour passed away, and slowly Dorothy got over her fright;\n",
        "but she felt quite lonely, and the wind shrieked so loudly all about\n",
        "her that she nearly became deaf. At first she had wondered if she would\n",
        "be dashed to pieces when the house fell again; but as the hours passed\n",
        "and nothing terrible happened, she stopped worrying and resolved to\n",
        "wait calmly and see what the future would bring. At last she crawled\n",
        "over the swaying floor to her bed, and lay down upon it; and Toto\n",
        "followed and lay down beside her.\n",
        "\n",
        "In spite of the swaying of the house and the wailing of the wind,\n",
        "Dorothy soon closed her eyes and fell fast asleep.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b8b764",
      "metadata": {},
      "source": [
        "In order to create embeddings of our text, we have to break it down. I'm splitting the text by sentence in the hope that it gives us reasonable sizes.\n",
        "\n",
        "We're using langchain's CharacterTextSplitter here to do the splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b2a0b58c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2a0b58c",
        "outputId": "75bc2da1-9411-4748-f3c2-31eb5998c6a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 303, which is longer than the specified 250\n",
            "Created a chunk of size 292, which is longer than the specified 250\n"
          ]
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter(separator= \".\", chunk_size=250, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_text(woz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "58225a20",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Dorothy lived in the midst of the great Kansas prairies, with Uncle\\nHenry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\\nhouse was small, for the lumber to build it had to be carried by wagon\\nmany miles'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's look at the first split. Note that it split the input at a sentence boundary, but not at every sentence boundary. It does try to fill the chunk size.\n",
        "all_splits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8e1ac3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# What is the embedding generated from that chunk?\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "embeddings.embed_query(all_splits[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1622359",
      "metadata": {},
      "source": [
        "This is almost too easy because a lot is happening behind the scenes that is hidden from us. \n",
        "\n",
        "1. We're using the LangChain HuggingFaceEmbeddings class. It uses BAAI/bge-large-en-v1.5 to generate the embeddings by default. More info is at https://huggingface.co/BAAI/bge-large-en-v1.5\n",
        "2. It uses chromadb to convert the text chunks into embeddings and load them into a local database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "61b26f9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b26f9d",
        "outputId": "bb3423a6-b535-4b51-a976-4595e84dbc14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)99753/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 8.54MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.45MB/s]\n",
            "Downloading (…)0cdb299753/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 43.8MB/s]\n",
            "Downloading (…)db299753/config.json: 100%|██████████| 571/571 [00:00<00:00, 4.34MB/s]\n",
            "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 572kB/s]\n",
            "Downloading (…)753/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 114MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:01<00:00, 233MB/s] \n",
            "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 197kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 762kB/s]\n",
            "Downloading (…)99753/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 27.6MB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 1.14MB/s]\n",
            "Downloading (…)9753/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 21.2MB/s]\n",
            "Downloading (…)0cdb299753/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 99.2MB/s]\n",
            "Downloading (…)b299753/modules.json: 100%|██████████| 349/349 [00:00<00:00, 2.64MB/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "\u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/llm-demos/chat_with_document/question_answer.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings \u001b[39m=\u001b[39m HuggingFaceEmbeddings()\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m docsearch \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_texts(all_splits, embeddings, metadatas\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mstr\u001b[39;49m(i)} \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(all_splits))])\u001b[39m.\u001b[39mas_retriever()\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print the first document's embedding\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Beffective-adventure-q75gw799r2xj4/workspaces/llm-demos/chat_with_document/question_answer.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m docsearch\u001b[39m.\u001b[39membeddings[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain/vectorstores/chroma.py:620\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    588\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    589\u001b[0m     \u001b[39mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    600\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Chroma:\n\u001b[1;32m    601\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \n\u001b[1;32m    603\u001b[0m \u001b[39m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    621\u001b[0m         collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[1;32m    622\u001b[0m         embedding_function\u001b[39m=\u001b[39;49membedding,\n\u001b[1;32m    623\u001b[0m         persist_directory\u001b[39m=\u001b[39;49mpersist_directory,\n\u001b[1;32m    624\u001b[0m         client_settings\u001b[39m=\u001b[39;49mclient_settings,\n\u001b[1;32m    625\u001b[0m         client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m    626\u001b[0m         collection_metadata\u001b[39m=\u001b[39;49mcollection_metadata,\n\u001b[1;32m    627\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m         ids \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid1()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m texts]\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain/vectorstores/chroma.py:80\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize with a Chroma client.\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/chromadb/__init__.py:78\u001b[0m\n\u001b[1;32m     76\u001b[0m             sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39msqlite3\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mpysqlite3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mYour system has an unsupported version of sqlite3. Chroma \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m                    requires sqlite3 >= 3.35.0.\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[94mPlease visit \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m                    https://docs.trychroma.com/troubleshooting#sqlite to learn how \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m                    to upgrade.\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Override Chroma's default settings, environment variables or .env files\"\"\"\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m"
          ]
        }
      ],
      "source": [
        "docsearch = Chroma.from_texts(all_splits, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(all_splits))]).as_retriever()\n",
        "\n",
        "print(f\"there are {len(all_splits)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e3d84d",
      "metadata": {},
      "source": [
        "And this is how we ask the document a question.\n",
        "\n",
        "1. We load a QA chain using langchain. This is a simple \"stuff\" chain that just crams the documents into the buffer and asks the question\n",
        "\n",
        "See https://python.langchain.com/docs/modules/chains/document/stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1404fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1404fc5",
        "outputId": "7b8c6c86-0449-49df-d204-b49318a43a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='It was reached by a trap door in the\\nmiddle of the floor, from which a ladder led down into the small, dark\\nhole.\\n\\nWhen Dorothy stood in the doorway and looked around, she could see\\nnothing but the great gray prairie on every side' metadata={'source': '4'}\n",
            "page_content='There was no garret at all, and no cellar—except a\\nsmall hole dug in the ground, called a cyclone cellar, where the family\\ncould go in case one of those great whirlwinds arose, mighty enough to\\ncrush any building in its path' metadata={'source': '3'}\n",
            "page_content='Dorothy lived in the midst of the great Kansas prairies, with Uncle\\nHenry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\\nhouse was small, for the lumber to build it had to be carried by wagon\\nmany miles' metadata={'source': '0'}\n",
            "page_content='There were four walls, a floor and a roof, which made one\\nroom; and this room contained a rusty looking cookstove, a cupboard for\\nthe dishes, a table, three or four chairs, and the beds' metadata={'source': '1'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "chain = load_qa_chain(local_llm, chain_type=\"stuff\")\n",
        "\n",
        "query = \"Where is the farmhouse?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "# ask the chain our questions and give it the most relevant documents\n",
        "chain(query, docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83435f41",
      "metadata": {},
      "source": [
        "There are 4 document chain strategies.\n",
        "\n",
        "1. Stuff. The simplest one. Shown above.\n",
        "2. The Refine documents chain constructs a response by looping over the input documents and iteratively updating its answer.\n",
        "3. The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step).\n",
        "4. The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned.\n",
        "\n",
        "I find that \"refine\" generally gives good quality answers and runs a little faster than \"map reduce\" and \"map re-rank.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2imwL1YUO5XT",
      "metadata": {
        "id": "2imwL1YUO5XT"
      },
      "outputs": [],
      "source": [
        "refine_prompt_template = (\n",
        "    \"The original question is as follows: {question}\\n\"\n",
        "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question. \"\n",
        "    \"If the context isn't useful, return the original answer.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
        "    template=refine_prompt_template,\n",
        ")\n",
        "\n",
        "\n",
        "initial_qa_template = (\n",
        "    \"Context information is below. \\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question: {question}\\n\"\n",
        ")\n",
        "initial_qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context_str\", \"question\"], template=initial_qa_template\n",
        ")\n",
        "chain = load_qa_chain(local_llm, chain_type=\"refine\", return_refine_steps=True,\n",
        "                     question_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b28d220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b28d220",
        "outputId": "7449f77e-727f-4127-8f8f-d7a823a93085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Then a strange thing happened.\\n\\nThe house whirled around two or three times and rose slowly through the\\nair. Dorothy felt as if she were going up in a balloon' metadata={'source': '19'}\n",
            "page_content='After the first few whirls around,\\nand one other time when the house tipped badly, she felt as if she were\\nbeing rocked gently, like a baby in a cradle.\\n\\nToto did not like it' metadata={'source': '23'}\n",
            "page_content='The north and south winds met where the house stood, and made it the\\nexact center of the cyclone' metadata={'source': '20'}\n",
            "page_content='There were four walls, a floor and a roof, which made one\\nroom; and this room contained a rusty looking cookstove, a cupboard for\\nthe dishes, a table, three or four chairs, and the beds' metadata={'source': '1'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': ['The house whirled around two or three times.',\n",
              "  'The house whirled around two or three times.',\n",
              "  'The house whirled around two or three times.',\n",
              "  'The house spun around two or three times.'],\n",
              " 'output_text': 'The house spun around two or three times.'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How many times did the house spin?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2PmHgNsOShwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PmHgNsOShwl",
        "outputId": "2cc79858-b088-45c7-8cb8-4f0b23a45b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Dorothy lived in the midst of the great Kansas prairies, with Uncle\\nHenry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\\nhouse was small, for the lumber to build it had to be carried by wagon\\nmany miles' metadata={'source': '0'}\n",
            "page_content='Then a strange thing happened.\\n\\nThe house whirled around two or three times and rose slowly through the\\nair. Dorothy felt as if she were going up in a balloon' metadata={'source': '19'}\n",
            "page_content='“I’ll go look\\nafter the stock.” Then he ran toward the sheds where the cows and\\nhorses were kept.\\n\\nAunt Em dropped her work and came to the door. One glance told her of\\nthe danger close at hand.\\n\\n“Quick, Dorothy!” she screamed' metadata={'source': '16'}\n",
            "page_content='Uncle Henry\\nand Aunt Em had a big bed in one corner, and Dorothy a little bed in\\nanother corner' metadata={'source': '2'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': [\"Dorothy's house was in the midst of the great Kansas prairies.\",\n",
              "  \"Dorothy's house was in the midst of the great Kansas prairies.\",\n",
              "  \"Dorothy's house was in the midst of the great Kansas prairies.\",\n",
              "  \"Dorothy's house was in the midst of the great Kansas prairies.\"],\n",
              " 'output_text': \"Dorothy's house was in the midst of the great Kansas prairies.\"}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Where was Dorothy's house?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccxNAOR9TAuW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccxNAOR9TAuW",
        "outputId": "7a59eaeb-5b1e-44d2-eae2-4686f260ec1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Uncle Henry\\nand Aunt Em had a big bed in one corner, and Dorothy a little bed in\\nanother corner' metadata={'source': '2'}\n",
            "page_content='There were four walls, a floor and a roof, which made one\\nroom; and this room contained a rusty looking cookstove, a cupboard for\\nthe dishes, a table, three or four chairs, and the beds' metadata={'source': '1'}\n",
            "page_content='Dorothy lived in the midst of the great Kansas prairies, with Uncle\\nHenry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\\nhouse was small, for the lumber to build it had to be carried by wagon\\nmany miles' metadata={'source': '0'}\n",
            "page_content='Once the house had\\nbeen painted, but the sun blistered the paint and the rains washed it\\naway, and now the house was as dull and gray as everything else.\\n\\nWhen Aunt Em came there to live she was a young, pretty wife' metadata={'source': '7'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': ['Uncle Henry, Aunt Em, Dorothy',\n",
              "  'The people who lived in the house were Uncle Henry, Aunt Em, Dorothy.',\n",
              "  'The people who lived in the house were Uncle Henry, Aunt Em, and Dorothy.',\n",
              "  'The people who lived in the house were Uncle Henry, Aunt Em, and Dorothy.'],\n",
              " 'output_text': 'The people who lived in the house were Uncle Henry, Aunt Em, and Dorothy.'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"List the people who lived in the house\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ebMVdQ4aTIy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebMVdQ4aTIy",
        "outputId": "4ee7f8a5-f85f-4cee-a2d6-e3019a3bad94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='In the middle of a cyclone the air is\\ngenerally still, but the great pressure of the wind on every side of\\nthe house raised it up higher and higher, until it was at the very top\\nof the cyclone; and there it remained and was carried miles and miles\\naway as easily as you could carry a feather' metadata={'source': '21'}\n",
            "page_content='Dorothy lived in the midst of the great Kansas prairies, with Uncle\\nHenry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\\nhouse was small, for the lumber to build it had to be carried by wagon\\nmany miles' metadata={'source': '0'}\n",
            "page_content='Then a strange thing happened.\\n\\nThe house whirled around two or three times and rose slowly through the\\nair. Dorothy felt as if she were going up in a balloon' metadata={'source': '19'}\n",
            "page_content='The north and south winds met where the house stood, and made it the\\nexact center of the cyclone' metadata={'source': '20'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': ['The house was carried miles and miles away.',\n",
              "  \"The house was carried miles and miles away from Dorothy's house.\",\n",
              "  \"The house was carried miles and miles away from Dorothy's house.\",\n",
              "  \"The house was carried miles and miles away from Dorothy's house.\"],\n",
              " 'output_text': \"The house was carried miles and miles away from Dorothy's house.\"}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How far was the house carried?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
