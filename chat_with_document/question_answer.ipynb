{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshtimmons/llm-demos/blob/main/chat_with_document/question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9528f94",
      "metadata": {},
      "source": [
        "# How to \"Chat\" with a document\n",
        "\n",
        "This is the accompanying material for the 11/8 AI session. \n",
        "\n",
        "In this session we will learn about:\n",
        "1. text embeddings\n",
        "2. vector stores\n",
        "3. document chunking\n",
        "4. document retrieval based on a prompted question\n",
        "5. converting a retrieved document into an \"answer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ji2q5P50hyC6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji2q5P50hyC6",
        "outputId": "5d2bcce4-0150-4db4-c557-e8761b1cd974"
      },
      "outputs": [],
      "source": [
        "!pip install langchain transformers sentence_transformers \"chromadb<=0.4.15\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9908d800",
      "metadata": {},
      "source": [
        "First we're going to do some basic model setup. I'm using the lightweight flan-alpaca-large model that is tuned for assistive generation. \n",
        "\n",
        "This model plays two roles in our QA scenario:\n",
        "1. It will figure out what text in our documents contains content that answers the question\n",
        "2. It will combine the information we collect into a single answer\n",
        "\n",
        "This is a small model by LLM standards at 0.7B parameters, but it's good enough for the job :-) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b2d99a",
      "metadata": {
        "id": "d5b2d99a"
      },
      "outputs": [],
      "source": [
        "# This code downloads the model from the HuggingFace model hub and loads it into a pipeline.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "model_id = \"declare-lab/flan-alpaca-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30105807",
      "metadata": {},
      "source": [
        "Next we're going to start the process of vectorizing and storing the text.\n",
        "\n",
        "I'm using the first chapter of \"The Wizard of Oz\" by Frank Baum for illustrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965555f4",
      "metadata": {
        "id": "965555f4"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PqW5bk6GnEpD",
      "metadata": {
        "id": "PqW5bk6GnEpD"
      },
      "outputs": [],
      "source": [
        "woz = \"\"\"\n",
        "Dorothy lived in the midst of the great Kansas prairies, with Uncle\n",
        "Henry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\n",
        "house was small, for the lumber to build it had to be carried by wagon\n",
        "many miles. There were four walls, a floor and a roof, which made one\n",
        "room; and this room contained a rusty looking cookstove, a cupboard for\n",
        "the dishes, a table, three or four chairs, and the beds. Uncle Henry\n",
        "and Aunt Em had a big bed in one corner, and Dorothy a little bed in\n",
        "another corner. There was no garret at all, and no cellar—except a\n",
        "small hole dug in the ground, called a cyclone cellar, where the family\n",
        "could go in case one of those great whirlwinds arose, mighty enough to\n",
        "crush any building in its path. It was reached by a trap door in the\n",
        "middle of the floor, from which a ladder led down into the small, dark\n",
        "hole.\n",
        "\n",
        "When Dorothy stood in the doorway and looked around, she could see\n",
        "nothing but the great gray prairie on every side. Not a tree nor a\n",
        "house broke the broad sweep of flat country that reached to the edge of\n",
        "the sky in all directions. The sun had baked the plowed land into a\n",
        "gray mass, with little cracks running through it. Even the grass was\n",
        "not green, for the sun had burned the tops of the long blades until\n",
        "they were the same gray color to be seen everywhere. Once the house had\n",
        "been painted, but the sun blistered the paint and the rains washed it\n",
        "away, and now the house was as dull and gray as everything else.\n",
        "\n",
        "When Aunt Em came there to live she was a young, pretty wife. The sun\n",
        "and wind had changed her, too. They had taken the sparkle from her eyes\n",
        "and left them a sober gray; they had taken the red from her cheeks and\n",
        "lips, and they were gray also. She was thin and gaunt, and never smiled\n",
        "now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
        "been so startled by the child’s laughter that she would scream and\n",
        "press her hand upon her heart whenever Dorothy’s merry voice reached\n",
        "her ears; and she still looked at the little girl with wonder that she\n",
        "could find anything to laugh at.\n",
        "\n",
        "Uncle Henry never laughed. He worked hard from morning till night and\n",
        "did not know what joy was. He was gray also, from his long beard to his\n",
        "rough boots, and he looked stern and solemn, and rarely spoke.\n",
        "\n",
        "It was Toto that made Dorothy laugh, and saved her from growing as gray\n",
        "as her other surroundings. Toto was not gray; he was a little black\n",
        "dog, with long silky hair and small black eyes that twinkled merrily on\n",
        "either side of his funny, wee nose. Toto played all day long, and\n",
        "Dorothy played with him, and loved him dearly.\n",
        "\n",
        "Today, however, they were not playing. Uncle Henry sat upon the\n",
        "doorstep and looked anxiously at the sky, which was even grayer than\n",
        "usual. Dorothy stood in the door with Toto in her arms, and looked at\n",
        "the sky too. Aunt Em was washing the dishes.\n",
        "\n",
        "From the far north they heard a low wail of the wind, and Uncle Henry\n",
        "and Dorothy could see where the long grass bowed in waves before the\n",
        "coming storm. There now came a sharp whistling in the air from the\n",
        "south, and as they turned their eyes that way they saw ripples in the\n",
        "grass coming from that direction also.\n",
        "\n",
        "Suddenly Uncle Henry stood up.\n",
        "\n",
        "“There’s a cyclone coming, Em,” he called to his wife. “I’ll go look\n",
        "after the stock.” Then he ran toward the sheds where the cows and\n",
        "horses were kept.\n",
        "\n",
        "Aunt Em dropped her work and came to the door. One glance told her of\n",
        "the danger close at hand.\n",
        "\n",
        "“Quick, Dorothy!” she screamed. “Run for the cellar!”\n",
        "\n",
        "Toto jumped out of Dorothy’s arms and hid under the bed, and the girl\n",
        "started to get him. Aunt Em, badly frightened, threw open the trap door\n",
        "in the floor and climbed down the ladder into the small, dark hole.\n",
        "Dorothy caught Toto at last and started to follow her aunt. When she\n",
        "was halfway across the room there came a great shriek from the wind,\n",
        "and the house shook so hard that she lost her footing and sat down\n",
        "suddenly upon the floor.\n",
        "\n",
        "Then a strange thing happened.\n",
        "\n",
        "The house whirled around two or three times and rose slowly through the\n",
        "air. Dorothy felt as if she were going up in a balloon.\n",
        "\n",
        "The north and south winds met where the house stood, and made it the\n",
        "exact center of the cyclone. In the middle of a cyclone the air is\n",
        "generally still, but the great pressure of the wind on every side of\n",
        "the house raised it up higher and higher, until it was at the very top\n",
        "of the cyclone; and there it remained and was carried miles and miles\n",
        "away as easily as you could carry a feather.\n",
        "\n",
        "It was very dark, and the wind howled horribly around her, but Dorothy\n",
        "found she was riding quite easily. After the first few whirls around,\n",
        "and one other time when the house tipped badly, she felt as if she were\n",
        "being rocked gently, like a baby in a cradle.\n",
        "\n",
        "Toto did not like it. He ran about the room, now here, now there,\n",
        "barking loudly; but Dorothy sat quite still on the floor and waited to\n",
        "see what would happen.\n",
        "\n",
        "Once Toto got too near the open trap door, and fell in; and at first\n",
        "the little girl thought she had lost him. But soon she saw one of his\n",
        "ears sticking up through the hole, for the strong pressure of the air\n",
        "was keeping him up so that he could not fall. She crept to the hole,\n",
        "caught Toto by the ear, and dragged him into the room again, afterward\n",
        "closing the trap door so that no more accidents could happen.\n",
        "\n",
        "Hour after hour passed away, and slowly Dorothy got over her fright;\n",
        "but she felt quite lonely, and the wind shrieked so loudly all about\n",
        "her that she nearly became deaf. At first she had wondered if she would\n",
        "be dashed to pieces when the house fell again; but as the hours passed\n",
        "and nothing terrible happened, she stopped worrying and resolved to\n",
        "wait calmly and see what the future would bring. At last she crawled\n",
        "over the swaying floor to her bed, and lay down upon it; and Toto\n",
        "followed and lay down beside her.\n",
        "\n",
        "In spite of the swaying of the house and the wailing of the wind,\n",
        "Dorothy soon closed her eyes and fell fast asleep.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b8b764",
      "metadata": {},
      "source": [
        "In order to create embeddings of our text, we have to break it down. I'm splitting the text by sentence in the hope that it gives us reasonable sizes.\n",
        "\n",
        "We're using langchain's CharacterTextSplitter here to do the splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a0b58c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2a0b58c",
        "outputId": "75bc2da1-9411-4748-f3c2-31eb5998c6a2"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(separator= \".\", chunk_size=250, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_text(woz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58225a20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at the first split. Note that it split the input at a sentence boundary, but not at every sentence boundary. It does try to fill the chunk size.\n",
        "all_splits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8e1ac3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# What is the embedding generated from that chunk?\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "embeddings.embed_query(all_splits[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1622359",
      "metadata": {},
      "source": [
        "This is almost too easy because a lot is happening behind the scenes that is hidden from us. \n",
        "\n",
        "1. We're using the LangChain HuggingFaceEmbeddings class. It uses BAAI/bge-large-en-v1.5 to generate the embeddings by default. More info is at https://huggingface.co/BAAI/bge-large-en-v1.5\n",
        "2. It uses chromadb to convert the text chunks into embeddings and load them into a local database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b26f9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b26f9d",
        "outputId": "bb3423a6-b535-4b51-a976-4595e84dbc14"
      },
      "outputs": [],
      "source": [
        "docsearch = Chroma.from_texts(all_splits, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(all_splits))]).as_retriever()\n",
        "\n",
        "print(f\"there are {len(all_splits)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e3d84d",
      "metadata": {},
      "source": [
        "And this is how we ask the document a question.\n",
        "\n",
        "1. We load a QA chain using langchain. This is a simple \"stuff\" chain that just crams the documents into the buffer and asks the question\n",
        "\n",
        "See https://python.langchain.com/docs/modules/chains/document/stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1404fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1404fc5",
        "outputId": "7b8c6c86-0449-49df-d204-b49318a43a09"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "chain = load_qa_chain(local_llm, chain_type=\"stuff\")\n",
        "\n",
        "query = \"Where is the farmhouse?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "# ask the chain our questions and give it the most relevant documents\n",
        "chain({\"input_documents\": docs, \"question\": query})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83435f41",
      "metadata": {},
      "source": [
        "There are 4 document chain strategies.\n",
        "\n",
        "1. Stuff. The simplest one. Shown above.\n",
        "2. The Refine documents chain constructs a response by looping over the input documents and iteratively updating its answer.\n",
        "3. The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step).\n",
        "4. The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned.\n",
        "\n",
        "I find that \"refine\" generally gives good quality answers and runs a little faster than \"map reduce\" and \"map re-rank.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2imwL1YUO5XT",
      "metadata": {
        "id": "2imwL1YUO5XT"
      },
      "outputs": [],
      "source": [
        "refine_prompt_template = (\n",
        "    \"The original question is as follows: {question}\\n\"\n",
        "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question. If the context doesn't answer the question, say you don't know. Do not make up an answer.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
        "    template=refine_prompt_template,\n",
        ")\n",
        "\n",
        "\n",
        "initial_qa_template = (\n",
        "    \"Context information is below. \\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question. Do not make up an answer if you do not know: {question}\\n\"\n",
        ")\n",
        "initial_qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context_str\", \"question\"], template=initial_qa_template\n",
        ")\n",
        "chain = load_qa_chain(local_llm, chain_type=\"refine\", return_refine_steps=True,\n",
        "                     question_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b28d220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b28d220",
        "outputId": "7449f77e-727f-4127-8f8f-d7a823a93085"
      },
      "outputs": [],
      "source": [
        "query = \"How many times did the house spin?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2PmHgNsOShwl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PmHgNsOShwl",
        "outputId": "2cc79858-b088-45c7-8cb8-4f0b23a45b18"
      },
      "outputs": [],
      "source": [
        "query = \"Where was Dorothy's house?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccxNAOR9TAuW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccxNAOR9TAuW",
        "outputId": "7a59eaeb-5b1e-44d2-eae2-4686f260ec1a"
      },
      "outputs": [],
      "source": [
        "query = \"List the people who lived in the house\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ebMVdQ4aTIy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebMVdQ4aTIy",
        "outputId": "4ee7f8a5-f85f-4cee-a2d6-e3019a3bad94"
      },
      "outputs": [],
      "source": [
        "query = \"How far was the house carried?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "for d in docs:\n",
        "  print(d)\n",
        "\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
